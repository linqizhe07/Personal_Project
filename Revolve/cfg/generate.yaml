# config file for reward generation from GPT*

defaults:
  - config
  - _self_

root_dir: ${oc.env:ROOT_PATH}

database:
  # folders
  rewards_dir: ${root_dir}/database/${evolution.baseline}/${data_paths.run}  # store path for rewards database
  # hyper params
  num_islands: 4  # number of groups/populations to start with
  max_island_size: 8  # max number of samples in each group
#  reset_period: 8  # reset the lowest performing group after period (iterations)
  crossover_prob: 0.5  # probability of preferring a crossover over mutation
  migration_prob: 0.3  # probability of resetting weaker islands with seeds migrated from other islands
  # sampling hyperparameters
  initial_temp: 1  # start with a higher temperature for uniform sampling (exploration)
  final_temp: 1  # exploitation
  num_gpus: 1  # number of gpus to train policies on using parallel processing

evolution:
  num_generations: 7  # number of iterations T
  baseline: revolve_auto  # revolve, revolve_auto, eureka, eureka_auto
  individuals_per_generation: 15  # number of individuals in each generation

few_shot:
  mutation: 1  # few-shot prompting for mutation
  crossover: 2  # few-shot prompting for crossover

data_paths:
  log: True  # log metrics and stats
  run: 10
  output_logs: ${root_dir}/logs/${evolution.baseline}/${data_paths.run}

environment:
  name: "HumanoidEnv"  # Choose between "HumanoidEnv" or "AdroitHandDoorEnv"

u2o:
  enabled: false  # set to true to use U2O pretraining
  pretrained_dir: ${root_dir}/u2o_pretrained
  feature_dim: 32
  bridge:
    regularization: 1e-4

